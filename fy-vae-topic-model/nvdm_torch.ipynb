{"cells":[{"cell_type":"code","execution_count":1,"id":"2e2ce647","metadata":{},"outputs":[],"source":["import os\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""]},{"cell_type":"code","execution_count":null,"id":"475c2ce3","metadata":{},"outputs":[],"source":["import os\n","\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from pathlib import Path\n","from losses import entmax_loss, entmax15\n","import optuna\n","import pickle\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from helper import log_stdout\n","import sys\n","import os"]},{"cell_type":"code","execution_count":3,"id":"922ee126-901c-47b0-90aa-bc42841a644e","metadata":{"id":"922ee126-901c-47b0-90aa-bc42841a644e"},"outputs":[],"source":["#-------------------------------\n","def data_set(data_url):\n","    \"\"\"process data input.\"\"\"\n","    data = []\n","    word_count = []\n","    fin = open(data_url)\n","    while True:\n","        line = fin.readline()\n","        if not line:\n","            break\n","        id_freqs = line.split()\n","        doc = {}\n","        count = 0\n","        for id_freq in id_freqs[1:]:\n","            items = id_freq.split(':')\n","            # python starts from 0\n","            doc[int(items[0])-1] = int(items[1])\n","            count += int(items[1])\n","        if count > 0:\n","            data.append(doc)\n","            word_count.append(count)\n","    fin.close()\n","    return data, word_count\n","\n","\n","def create_batches(data_size, batch_size, shuffle=True):\n","    \"\"\"create index by batches.\"\"\"\n","    batches = []\n","    ids = list(range(data_size))\n","    if shuffle:\n","        random.shuffle(ids)\n","    for i in list(range(data_size // batch_size)):\n","        start = i * batch_size\n","        end = (i + 1) * batch_size\n","        batches.append(ids[start:end])\n","  # the batch of which the length is less than batch_size\n","    rest = data_size % batch_size\n","    if rest > 0:\n","        batches.append(ids[-rest:] + [-1] * (batch_size - rest))  # -1 as padding\n","    return batches\n","\n","\n","def fetch_data(data, count, idx_batch, vocab_size):\n","    \"\"\"fetch input data by batch.\"\"\"\n","    batch_size = len(idx_batch)\n","    data_batch = np.zeros((batch_size, vocab_size))\n","    count_batch = []\n","    mask = np.zeros(batch_size)\n","    for i, doc_id in enumerate(idx_batch):\n","        if doc_id != -1:\n","            for word_id, freq in data[doc_id].items():\n","                data_batch[i, word_id] = freq\n","            count_batch.append(count[doc_id])\n","            mask[i]=1.0\n","        else:\n","            count_batch.append(0)\n","    return data_batch, count_batch, mask"]},{"cell_type":"code","execution_count":4,"id":"778c57c3-fd29-402b-86f7-5d55d7015fe9","metadata":{"id":"778c57c3-fd29-402b-86f7-5d55d7015fe9"},"outputs":[],"source":["from spcdist.torch import MultivariateBetaGaussianDiag\n","#-------------------------------\n","class Encoder(nn.Module):\n","    def __init__(self, args):\n","        super(Encoder, self).__init__()\n","        self.args = args\n","        self.mlp = nn.Linear(args['n_input'], args['n_hidden'])\n","        self.mean_fc = nn.Linear(args['n_hidden'], args['n_topics'])\n","        self.logsigm_fc = nn.Linear(args['n_hidden'], args['n_topics'])\n","        nn.init.zeros_(self.logsigm_fc.weight)  # cf. https://github.com/ysmiao/nvdm/blob/master/nvdm.py#L51\n","        nn.init.zeros_(self.logsigm_fc.bias)\n","\n","    def forward(self, doc_freq_vecs, mask):\n","\n","        en_vec = F.tanh(self.mlp(doc_freq_vecs))\n","        mean = self.mean_fc(en_vec)\n","        logsigm = self.logsigm_fc(en_vec) \n","        \n","        if self.args['normal']=='normal':\n","            kld = -0.5 * torch.sum(1 - torch.square(mean) + 2 * logsigm - (2 * logsigm).exp(), 1)\n","            return mask*kld, mean, logsigm, None\n","\n","        else:\n","            sigma2 = torch.exp(logsigm)\n","            mvbg = MultivariateBetaGaussianDiag(mean, sigma2, alpha=self.args['beta'])\n","            r = torch.exp(mvbg.log_radius).cuda()\n","            n = mvbg._fact_scale.rank \n","            first_term = 0.5 * torch.sum(mean ** 2, dim=1) \n","            numerator = r ** 2\n","            denominator = 2 * self.args['beta'] + n * (self.args['beta'] - 1)  \n","\n","            determinant = torch.prod(sigma2, dim=1)  # [64]\n","            exponent = -1 / (n + 2 / (self.args['beta'] - 1))  \n","\n","            coefficient = 0.5 * (self.args['beta'] - 1) \n","\n","            second_term = (numerator / denominator) * (determinant + 1e-8 )  ** exponent * (\n","                (1 + coefficient * torch.sum(sigma2, dim=1)) -\n","                (1 + 0.5 * n * (self.args['beta'] - 1))  )\n","            \n","            fy = first_term + second_term  \n","            return mask*fy, mean, logsigm, mvbg        \n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, args):\n","        super(Decoder, self).__init__()\n","        self.args = args\n","        self.decoder = nn.Linear(args['n_topics'], args['n_input'])  #\n","\n","    def forward(self, doc_freq_vecs, mean, logsigm, mvbg=None, train=True):\n","        if train:\n","            if self.args['normal'] == \"normal\":      \n","                eps = torch.randn(self.args['batch_size'], self.args['n_topics']).cuda()\n","                doc_vec = torch.mul(torch.exp(logsigm), eps) + mean\n","            else:\n","                assert mvbg is not None\n","                doc_vec = mvbg.rsample(sample_shape=(1,))[0]\n","            \n","            doc_vec = doc_vec.cuda()\n","            z = self.decoder(doc_vec)\n","            \n","            if self.args['loss'] == 'entmax':\n","                recon = entmax_loss(z, torch.tensor((doc_freq_vecs.T/doc_freq_vecs.sum(-1)), \n","                                                              dtype=torch.float).T, alpha=self.args['alpha'])*doc_freq_vecs.sum(-1) # 1.01 for sanity check\n","            else:\n","                logprobs = F.log_softmax(z, dim=1)\n","                recon = -torch.sum(torch.mul(logprobs, doc_freq_vecs), 1)\n","            return recon\n","        else:\n","            z = self.decoder(mean)\n","            temp = torch.tensor((doc_freq_vecs.T/doc_freq_vecs.sum(-1)), \n","                                                              dtype=torch.float).T\n","            if self.args['loss'] == 'entmax':\n","                recon =  torch.abs(temp-entmax15(z, dim=-1)).sum() \n","            else:\n","                recon =  torch.abs(temp-torch.softmax(z, dim=-1)).sum() \n","            return recon\n","\n","#-------------------------------\n","def make_optimizer(encoder, decoder, args):\n","    if args['optimizer'] == 'Adam':\n","        optimizer_enc = torch.optim.Adam(encoder.parameters(), args['learning_rate'], betas=(args['momentum'], 0.999))\n","        optimizer_dec = torch.optim.Adam(decoder.parameters(), args['learning_rate'], betas=(args['momentum'], 0.999))\n","\n","    elif args['optimizer'] == 'SGD':\n","        optimizer_enc = torch.optim.SGD(encoder.parameters(), args['learning_rate'], momentum=args['momentum'])\n","        optimizer_dec = torch.optim.SGD(decoder.parameters(), args['learning_rate'], momentum=args['momentum'])\n","\n","    return optimizer_enc, optimizer_dec\n"]},{"cell_type":"code","execution_count":5,"id":"0f10685e","metadata":{},"outputs":[],"source":["def run_eval(data_set, count, batches, encoder, decoder):\n","    \n","    encoder.eval()\n","    decoder.eval()\n","    with torch.no_grad():\n","        error_sum = 0.0\n","        # ppx_sum = 0.0\n","        kld_sum = 0.0\n","        word_count = 0\n","        doc_count = 0\n","\n","        for idx_batch in batches:\n","            data_batch, count_batch, mask = fetch_data(data_set, count, idx_batch, 2000)\n","            data_batch = torch.tensor(data_batch, dtype=torch.float).cuda()\n","            count_batch = torch.tensor(count_batch, dtype=torch.float).cuda()\n","            mask = torch.tensor(mask).cuda()\n","\n","            kld, mean, logsigm, _ = encoder(data_batch, mask)\n","            objective = decoder(data_batch, mean, logsigm, train=False)\n","\n","            # loss = objective + kld \n","\n","            error_sum += torch.sum(objective)\n","            kld_sum += (kld.sum() / mask.sum())\n","\n","            word_count += torch.sum(count_batch)\n","            count_batch = torch.add(count_batch, 1e-12)\n","            # ppx_sum += (loss/count_batch).sum()\n","            doc_count += mask.sum()\n","\n","        # print_kld = kld_sum / len(batches)\n","        # print_vppx = torch.exp(loss_sum / word_count)\n","        # print_ppx_perdoc = torch.exp(ppx_sum / doc_count)\n","\n","    return error_sum / len(batches), kld_sum / len(batches)"]},{"cell_type":"code","execution_count":6,"id":"97b6e9cb-5372-40ba-aaae-fa39567d8e9c","metadata":{"id":"97b6e9cb-5372-40ba-aaae-fa39567d8e9c"},"outputs":[],"source":["def train(params, data_dir, log_every=100, exp_dir=\"experiments_final\"):\n","\n","    args = {'batch_size': params['batch_size'],\n","              'optimizer': 'Adam',\n","              'learning_rate': params['learning_rate'],\n","              'momentum': 0.9,\n","              'n_epoch': params['n_epoch'],\n","              'n_alternating_epoch': params['n_alternating_epoch'],\n","              'init_mult': 0, # 0.001,\n","              'n_input': 2000,\n","              'n_topics': 50,\n","              'n_hidden': 500,\n","              'kld_weight': params['kld_weight'],\n","              'seed': params['seed'],\n","              'loss': LOSS,\n","              'alpha': params['alpha'],\n","              'normal': params['normal'],\n","              'beta': params['beta'],\n","              }\n","\n","    # exp_name = f\"{exp_dir}/lr{params['learning_rate']:.2e}_bs{params['batch_size']}_ae{params['n_alternating_epoch']}_np{params['n_epoch']}_kl{params['kld_weight']:.2e}_{LOSS}_{args['seed']}\"\n","    # os.makedirs(exp_name, exist_ok=True)\n","    # log_file = open(f\"{exp_name}/logs.txt\", \"w\")\n","\n","    torch.manual_seed(args['seed'])\n","\n","    train_url = os.path.join(data_dir, 'train.feat')\n","    train_set, train_count = data_set(train_url)\n","\n","    # test, dev batches\n","    test_url = os.path.join(data_dir, 'test.feat')\n","    test_set, test_count = data_set(test_url)\n","    dev_set = test_set[:1000]\n","    dev_count = test_count[:1000]\n","    test_set = test_set[1000:]\n","    test_count = test_count[1000:]\n","    \n","    test_batches = create_batches(len(test_set), len(test_set), shuffle=False)\n","    dev_batches = create_batches(len(dev_set),len(dev_set), shuffle=False)\n","\n","    print(len(test_set), len(dev_set), len(train_set))\n","\n","    # model\n","    encoder = Encoder(args)\n","    encoder.cuda()\n","    decoder = Decoder(args)\n","    decoder.cuda()\n","\n","    optimizer_enc, optimizer_dec = make_optimizer(encoder, decoder, args)\n","\n","    #-------------------------------\n","    # train\n","    for epoch in tqdm(range(args['n_epoch'])):\n","        train_batches = create_batches(len(train_set), args['batch_size'], shuffle=True)\n","        for switch in list(range(0, 2)):\n","            if switch == 0:\n","                optimizer = optimizer_dec\n","                decoder.train()\n","                print_mode = 'updating decoder'\n","            else:\n","                optimizer = optimizer_enc\n","                encoder.train()\n","                print_mode = 'updating encoder'\n","            for i in list(range(args['n_alternating_epoch'])):\n","                ppx_sum = 0.0\n","                kld_sum = 0.0\n","                word_count = 0\n","                doc_count = 0\n","                loss_sum = 0.0\n","\n","                for idx_batch in train_batches[:-1]:\n","                    data_batch, count_batch, mask = fetch_data(train_set, train_count, idx_batch, 2000)\n","                    data_batch = torch.tensor(data_batch, dtype=torch.float).cuda()\n","                    count_batch = torch.tensor(count_batch, dtype=torch.float).cuda()\n","                    mask = torch.tensor(mask).cuda()\n","\n","                    kld, mean, logsigm, mvbg = encoder(data_batch, mask)\n","                    objective = decoder(data_batch, mean, logsigm, mvbg)\n","\n","                    loss = objective + args['kld_weight'] * kld\n","\n","                    optimizer.zero_grad()\n","                    loss.mean().backward()\n","                    optimizer.step()\n","\n","                    loss_sum += torch.sum(loss)\n","                    kld_sum += (kld.sum() / mask.sum())\n","                    \n","                    word_count += torch.sum(count_batch)\n","                    # per document loss\n","                    count_batch = torch.add(count_batch, 1e-12)\n","                    ppx_sum += (loss.detach()/count_batch).sum()\n","                    doc_count += mask.sum()\n","\n","                print_kld = kld_sum/len(train_batches[:-1])\n","                print_ppx = torch.exp(loss_sum / word_count)\n","                print_ppx_perdoc = torch.exp(ppx_sum / doc_count)\n","\n","                if epoch % log_every==0:\n","                    print(f'| Epoch train: {epoch+1} | {i} | {print_mode} | Corpus ppx: {print_ppx:.5f} | Per doc ppx: {print_ppx_perdoc:.5f} | KLD: {print_kld:.5f}')\n","        \n","        #-------------------------------\n","        # dev\n","        if epoch % log_every==0:\n","            error, print_kld = run_eval(dev_set, dev_count, dev_batches, encoder, decoder)\n","            print(f'| Epoch eval: {epoch+1}| Reconst error: {error:.5f} | KLD: {print_kld:.5}')\n","            # torch.save(encoder.state_dict(), f\"{exp_name}/encoder_{epoch}.pkl\")\n","            # torch.save(decoder.state_dict(), f\"{exp_name}/decoder_{epoch}.pkl\")\n","            # with open(f\"{exp_name}/weights_{epoch}.pkl\", \"wb\") as f:\n","            #     pickle.dump(decoder.decoder.weight.cpu().data, f)\n","            \n","                \n","    #-------------------------------\n","    # test\n","    error, print_kld = run_eval(test_set, test_count, test_batches, encoder, decoder)\n","    print(f'| Epoch test: {epoch+1} | Reconst error: {error:.5f}| KLD: {print_kld:.5}')\n","    return error"]},{"cell_type":"code","execution_count":null,"id":"d248e5c0","metadata":{},"outputs":[],"source":["data_dir='./nvdm/data/20news/'\n","params = {\n","    'alpha': 1.6851816060492335, \n","    'batch_size': 64, # 64*16\n","    'kld_weight': 0.01,\n","    'learning_rate':  1e-05, # 5e-5*4\n","    'n_alternating_epoch': 1,\n","    'n_epoch': 500,\n","    'seed': random.randint(0, 100),\n","    'normal': 'q-normal',\n","    'beta': 2,\n","\n","}\n","LOSS='softmax'\n","train(params, data_dir, exp_dir=\"test\")"]},{"cell_type":"code","execution_count":null,"id":"666d0f0a-e995-4c8a-80d8-60e2953bf396","metadata":{"id":"666d0f0a-e995-4c8a-80d8-60e2953bf396","outputId":"448f0d46-1dd2-4682-992d-ace6d970e4c7","scrolled":true},"outputs":[],"source":["for i in range(5):\n","\n","    data_dir='./nvdm/data/20news/'\n","    params = {\n","        \n","        'alpha': 1.6851816060492335, \n","        'batch_size': 64, # 64*16\n","        'kld_weight': 0.01,\n","        'learning_rate':  5e-05, # 5e-5*4\n","        'n_alternating_epoch': 1,\n","        'n_epoch': 500,\n","        'seed': random.randint(0, 100),\n","    }\n","    LOSS='softmax'\n","    train(params, data_dir)"]},{"cell_type":"code","execution_count":null,"id":"2d93afc8","metadata":{},"outputs":[],"source":["import numpy as np\n","print(np.mean([x/6505 for x in [9759.68750, 9745.52148, 9744.94824, 9741.08203, 9746.99121]]))\n","print(np.std([x/6505 for x in [9759.68750, 9745.52148, 9744.94824, 9741.08203, 9746.99121]]))"]},{"cell_type":"code","execution_count":null,"id":"eddb0239","metadata":{},"outputs":[],"source":["for i in range(5):\n","\n","    data_dir='./nvdm/data/20news/'\n","    params = {\n","        'learning_rate':  5e-05, # 5e-5*4\n","        'batch_size': 1024, # 64*16\n","        'kld_weight': 0.01,\n","        'n_epoch': 500,\n","        'alpha': 1.3134782000900973, \n","        'n_alternating_epoch': 3,\n","        'seed': random.randint(0, 100),\n","\n","    }\n","\n","    LOSS='entmax'\n","    train(params, data_dir)"]},{"cell_type":"code","execution_count":null,"id":"e4191520","metadata":{},"outputs":[],"source":["import numpy as np\n","print(np.mean([x/6505 for x in [8716.68945, 8703.05859, 8701.54688, 8704.55957, 8706.33496]]))\n","print(np.std([x/6505 for x in [8716.68945, 8703.05859, 8701.54688, 8704.55957, 8706.33496]]))"]},{"cell_type":"markdown","id":"28e1ef6b","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"id":"3a696e56-957d-4115-8f60-21b4eb562511","metadata":{"id":"3a696e56-957d-4115-8f60-21b4eb562511","outputId":"810b9ec1-cbe9-47da-ed14-699acbcbe3c0","scrolled":true},"outputs":[],"source":["def vocab(data_url):\n","  \"\"\"process data input.\"\"\"\n","  data = []\n","  i2w = {}\n","  fin = open(data_url)\n","  while True:\n","    line = fin.readline()\n","    if not line:\n","      break\n","    word_freqs = line.split()\n","    index = len(data)\n","    data.append(word_freqs[0])\n","    i2w[index] = word_freqs[0]\n","\n","  fin.close()\n","  return i2w\n","\n","voc = vocab('nvdm/data/20news/vocab.new')\n","len(voc)"]},{"cell_type":"code","execution_count":4,"id":"0ab63588","metadata":{},"outputs":[],"source":["import pickle\n","with open(\"experiments/lr4.00e-04_bs4096_ae10_np1000_kl1.00e+00_entmax/weights_900.pkl\", \"rb\") as f:\n","    weights = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"27510974-70bd-4885-8ceb-b48d87aaf9fe","metadata":{"id":"27510974-70bd-4885-8ceb-b48d87aaf9fe","outputId":"a5e0d35f-eed6-4b89-8267-75653677b4fb"},"outputs":[],"source":["# check words most associated with latent dims\n","import torch\n","for t in range(50):\n","    words = torch.topk(weights, 10, 0)[1][:,t]\n","    topic_list = []\n","    for w in words:\n","        i2w = voc[w.item()]\n","        topic_list.append(i2w)\n","    print(topic_list)"]},{"cell_type":"code","execution_count":null,"id":"238ad253","metadata":{},"outputs":[],"source":["import pickle\n","with open(\"experiments/lr4.00e-04_bs4096_ae10_np1000_kl1.00e+00_softmax/weights_900.pkl\", \"rb\") as f:\n","    weights = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"7d40ae03","metadata":{},"outputs":[],"source":["# check words most associated with latent dims\n","import torch\n","for t in range(50):\n","    words = torch.topk(weights, 10, 0)[1][:,t]\n","    topic_list = []\n","    for w in words:\n","        i2w = voc[w.item()]\n","        topic_list.append(i2w)\n","    print(topic_list)"]},{"cell_type":"markdown","id":"c1f238d0","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":5}
