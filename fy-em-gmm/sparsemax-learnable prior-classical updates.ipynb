{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23fdf16-6e85-4cc1-90c8-13ace7eecd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 5 seeds:\n",
      "\n",
      "GMM adjusted MI: 0.6694 ± 0.0153\n",
      "GMM adjusted Rand Index: 0.6681 ± 0.0245\n",
      "GMM Silhouette Score: 0.4422 ± 0.0136\n"
     ]
    }
   ],
   "source": [
    "#fixed prior, \"classical\" sparsemax update\n",
    "\n",
    "import numpy as np \n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from torch.distributions import MultivariateNormal as N\n",
    "from entmax.activations import sparsemax, entmax15\n",
    "from entmax.root_finding import entmax_bisect\n",
    "\n",
    "# Set manual seeds for reproducibility\n",
    "t.manual_seed(40)\n",
    "np.random.seed(40)\n",
    "\n",
    "# Generate synthetic data once (consistent across seeds)\n",
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 2\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Generate blobs with centers close to each other\n",
    "centers = [[0, 0], [1, 1], [1, -1], [-1, -1]]\n",
    "cluster_std = [0.5, 0.7, 0.9, 0.11]  # Standard deviations\n",
    "\n",
    "X_np, y_np = make_blobs(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    centers=centers,\n",
    "    cluster_std=cluster_std,\n",
    "    random_state=0\n",
    ")\n",
    "X = t.tensor(X_np, dtype=t.float32)\n",
    "y = t.tensor(y_np, dtype=t.int64)\n",
    "\n",
    "# Add random noise points\n",
    "NUM_NOISE = 100\n",
    "noise = t.rand(NUM_NOISE, NUM_FEATURES) * 6 - 3  # Uniformly between -3 and 3\n",
    "X = t.cat([X, noise], dim=0)\n",
    "y = t.cat([y, t.full((NUM_NOISE,), -1, dtype=t.int64)], dim=0)  # Label noise points as -1\n",
    "\n",
    "# Constants\n",
    "BATCH = X.shape[0]\n",
    "DIM = X.shape[1]\n",
    "GUESS_CLASSES = 4\n",
    "\n",
    "# Define number of seeds\n",
    "num_seeds = 5\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Initialize accumulators for metrics\n",
    "metrics_gmm = {'ami': [], 'ari': [], 'silhouette': []}\n",
    "\n",
    "for seed in seeds:\n",
    "    # Set seed for reproducibility\n",
    "    t.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ############################\n",
    "    # Gaussian Mixture Model (GMM)\n",
    "    ############################\n",
    "    mu_gmm0 = t.rand(GUESS_CLASSES, DIM) * 0.1\n",
    "    # print(mu_gmm0)\n",
    "    s_gmm0 = t.rand(GUESS_CLASSES, DIM, DIM) * 0.1\n",
    "    s_gmm0 = s_gmm @ s_gmm.transpose(-2, -1) + t.einsum('ij,k->kij', t.eye(DIM), t.ones(GUESS_CLASSES))    \n",
    "    # to replicate the autodiff results that run all the models one after the other \n",
    "    \n",
    "    # Initialize parameters\n",
    "    mu_gmm = t.rand(GUESS_CLASSES, DIM) * 0.1\n",
    "    # print(mu_gmm)\n",
    "    s_gmm = t.rand(GUESS_CLASSES, DIM, DIM) * 0.1\n",
    "    s_gmm = s_gmm @ s_gmm.transpose(-2, -1) + t.einsum('ij,k->kij', t.eye(DIM), t.ones(GUESS_CLASSES))\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        # E-step\n",
    "        prior = t.distributions.Categorical(logits=t.zeros(GUESS_CLASSES))\n",
    "        dis = N(mu_gmm, s_gmm)\n",
    "        \n",
    "        log_p_x_given_z = dis.log_prob(X[:, None])  # Shape (BATCH, GUESS_CLASSES)\n",
    "        log_p_z = prior.probs[None, :]  # Shape (1, GUESS_CLASSES)\n",
    "        # print(prior.probs)\n",
    "        log_p_xz = log_p_x_given_z + log_p_z  # Shape (BATCH, GUESS_CLASSES)\n",
    "        \n",
    "        # Compute q_gmm (posterior probabilities)\n",
    "        # q_gmm = t.softmax(log_p_xz, dim=1).log()  # Shape (BATCH, GUESS_CLASSES)\n",
    "        q_gmm = entmax_bisect(log_p_xz, alpha=2, dim=-1)\n",
    "        \n",
    "        # M-step\n",
    "        mu_gmm = (q_gmm[:, :, None] * X[:, None, :]).sum(0) / q_gmm.sum(0)[:, None]\n",
    "        x_minus_mu = X[:, None, :] - mu_gmm[None, :, :]\n",
    "        s_gmm = ((x_minus_mu[:, :, :, None] @ x_minus_mu[:, :, None, :]) * q_gmm[:, :, None, None]).sum(0) / q_gmm.sum(0)[:, None, None]\n",
    "    \n",
    "    # Evaluate GMM\n",
    "    labels_gmm = q_gmm.argmax(1).numpy()\n",
    "\n",
    "    # Exclude noise points for evaluation\n",
    "    mask = y.numpy() != -1\n",
    "    y_eval = y.numpy()[mask]\n",
    "    labels_gmm_eval = labels_gmm[mask]\n",
    "    X_eval = X.numpy()[mask]\n",
    "    \n",
    "    # GMM Evaluation\n",
    "    metrics_gmm['ami'].append(adjusted_mutual_info_score(labels_gmm_eval, y_eval))\n",
    "    metrics_gmm['ari'].append(adjusted_rand_score(labels_gmm_eval, y_eval))\n",
    "    metrics_gmm['silhouette'].append(silhouette_score(X_eval, labels_gmm_eval))\n",
    "\n",
    "def average_metrics(metrics):\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "# Compute standard deviation metrics\n",
    "def std_metrics(metrics):\n",
    "    return {k: np.std(v) for k, v in metrics.items()}\n",
    "\n",
    "avg_gmm = average_metrics(metrics_gmm)\n",
    "std_gmm = std_metrics(metrics_gmm)\n",
    "\n",
    "# Print averaged metrics with standard deviation\n",
    "print('Averaged over 5 seeds:\\n')\n",
    "\n",
    "# GMM Evaluation\n",
    "print('GMM adjusted MI: {:.4f} ± {:.4f}'.format(avg_gmm['ami'], std_gmm['ami']))\n",
    "print('GMM adjusted Rand Index: {:.4f} ± {:.4f}'.format(avg_gmm['ari'], std_gmm['ari']))\n",
    "print('GMM Silhouette Score: {:.4f} ± {:.4f}'.format(avg_gmm['silhouette'], std_gmm['silhouette']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea9005c2-b619-4870-a547-0791f4cbf487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 5 seeds:\n",
      "\n",
      "GMM adjusted MI: 0.6702 ± 0.0152\n",
      "GMM adjusted Rand Index: 0.6679 ± 0.0246\n",
      "GMM Silhouette Score: 0.4422 ± 0.0133\n"
     ]
    }
   ],
   "source": [
    "# learnable prior, sparsemax update\n",
    "\n",
    "import numpy as np \n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from torch.distributions import MultivariateNormal as N\n",
    "\n",
    "# Set manual seeds for reproducibility\n",
    "t.manual_seed(40)\n",
    "np.random.seed(40)\n",
    "\n",
    "# Generate synthetic data once (consistent across seeds)\n",
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 2\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Generate blobs with centers close to each other\n",
    "centers = [[0, 0], [1, 1], [1, -1], [-1, -1]]\n",
    "cluster_std = [0.5, 0.7, 0.9, 0.11]  # Standard deviations\n",
    "\n",
    "X_np, y_np = make_blobs(\n",
    "    n_samples=NUM_SAMPLES,\n",
    "    centers=centers,\n",
    "    cluster_std=cluster_std,\n",
    "    random_state=0\n",
    ")\n",
    "X = t.tensor(X_np, dtype=t.float32)\n",
    "y = t.tensor(y_np, dtype=t.int64)\n",
    "\n",
    "# Add random noise points\n",
    "NUM_NOISE = 100\n",
    "noise = t.rand(NUM_NOISE, NUM_FEATURES) * 6 - 3  # Uniformly between -3 and 3\n",
    "X = t.cat([X, noise], dim=0)\n",
    "y = t.cat([y, t.full((NUM_NOISE,), -1, dtype=t.int64)], dim=0)  # Label noise points as -1\n",
    "\n",
    "# Constants\n",
    "BATCH = X.shape[0]\n",
    "DIM = X.shape[1]\n",
    "GUESS_CLASSES = 4\n",
    "\n",
    "# Define number of seeds\n",
    "num_seeds = 5\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Initialize accumulators for metrics\n",
    "metrics_gmm = {'ami': [], 'ari': [], 'silhouette': []}\n",
    "\n",
    "for seed in seeds:\n",
    "    # Set seed for reproducibility\n",
    "    t.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ############################\n",
    "    # Gaussian Mixture Model (GMM) with Trainable Prior\n",
    "    ############################\n",
    "    \n",
    "    # Initialize parameters\n",
    "    mu_gmm0 = t.rand(GUESS_CLASSES, DIM) * 0.1\n",
    "    # print(mu_gmm0)\n",
    "    s_gmm0 = t.rand(GUESS_CLASSES, DIM, DIM) * 0.1\n",
    "    s_gmm0 = s_gmm @ s_gmm.transpose(-2, -1) + t.einsum('ij,k->kij', t.eye(DIM), t.ones(GUESS_CLASSES))    \n",
    "    # to replicate the autodiff results that run all the models one after the other \n",
    "\n",
    "    mu_gmm = t.rand(GUESS_CLASSES, DIM) * 0.1\n",
    "    s_gmm = t.rand(GUESS_CLASSES, DIM, DIM) * 0.1\n",
    "    s_gmm = s_gmm @ s_gmm.transpose(-2, -1) + t.einsum('ij,k->kij', t.eye(DIM), t.ones(GUESS_CLASSES))\n",
    "    \n",
    "    # Initialize prior logits\n",
    "    prior_logits = t.zeros(GUESS_CLASSES, requires_grad=False)\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        # E-step\n",
    "        dis = N(mu_gmm, s_gmm)\n",
    "        log_p_x_given_z = dis.log_prob(X[:, None])  # Shape (BATCH, GUESS_CLASSES)\n",
    "        log_p_z = t.softmax(prior_logits, dim=0)[None, :]  # Shape (1, GUESS_CLASSES)\n",
    "        log_p_xz = log_p_x_given_z + log_p_z  # Shape (BATCH, GUESS_CLASSES)\n",
    "        \n",
    "        # Compute q_gmm (posterior probabilities)\n",
    "        # log_q_gmm = log_p_xz - t.logsumexp(log_p_xz, dim=1, keepdim=True)\n",
    "        # q_gmm = log_q_gmm.exp()  # Shape (BATCH, GUESS_CLASSES)\n",
    "\n",
    "        q_gmm = entmax_bisect(log_p_xz, alpha=2, dim=-1)  # Shape (BATCH, GUESS_CLASSES)\n",
    "        \n",
    "        # M-step\n",
    "        N_k = q_gmm.sum(dim=0)  # Effective number of data points assigned to each cluster\n",
    "        \n",
    "        # Update prior logits directly based on N_k\n",
    "        prior_logits = N_k.log()\n",
    "        \n",
    "        # Update means\n",
    "        mu_gmm = (q_gmm[:, :, None] * X[:, None, :]).sum(0) / N_k[:, None]\n",
    "        \n",
    "        # Update covariances\n",
    "        x_minus_mu = X[:, None, :] - mu_gmm[None, :, :]\n",
    "        s_gmm = ((q_gmm[:, :, None, None] * (x_minus_mu[:, :, :, None] * x_minus_mu[:, :, None, :])).sum(0)) / N_k[:, None, None]\n",
    "        \n",
    "        # Avoid singular covariance matrices\n",
    "        s_gmm += t.eye(DIM)[None, :, :] * 1e-6\n",
    "    \n",
    "    # Evaluate GMM\n",
    "    labels_gmm = q_gmm.argmax(1).numpy()\n",
    "\n",
    "    # Exclude noise points for evaluation\n",
    "    mask = y.numpy() != -1\n",
    "    y_eval = y.numpy()[mask]\n",
    "    labels_gmm_eval = labels_gmm[mask]\n",
    "    X_eval = X.numpy()[mask]\n",
    "    \n",
    "    # GMM Evaluation\n",
    "    metrics_gmm['ami'].append(adjusted_mutual_info_score(labels_gmm_eval, y_eval))\n",
    "    metrics_gmm['ari'].append(adjusted_rand_score(labels_gmm_eval, y_eval))\n",
    "    metrics_gmm['silhouette'].append(silhouette_score(X_eval, labels_gmm_eval))\n",
    "\n",
    "def average_metrics(metrics):\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "# Compute standard deviation metrics\n",
    "def std_metrics(metrics):\n",
    "    return {k: np.std(v) for k, v in metrics.items()}\n",
    "\n",
    "avg_gmm = average_metrics(metrics_gmm)\n",
    "std_gmm = std_metrics(metrics_gmm)\n",
    "\n",
    "# Print averaged metrics with standard deviation\n",
    "print('Averaged over 5 seeds:\\n')\n",
    "\n",
    "# GMM Evaluation\n",
    "print('GMM adjusted MI: {:.4f} ± {:.4f}'.format(avg_gmm['ami'], std_gmm['ami']))\n",
    "print('GMM adjusted Rand Index: {:.4f} ± {:.4f}'.format(avg_gmm['ari'], std_gmm['ari']))\n",
    "print('GMM Silhouette Score: {:.4f} ± {:.4f}'.format(avg_gmm['silhouette'], std_gmm['silhouette']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b8467-2e8a-44a5-a3f3-577dc9a7e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable prior\n",
    "\n",
    "# p_z\n",
    "# Averaged over 5 seeds:\n",
    "\n",
    "# Sparse EM GMM adjusted MI: 0.6702 ± 0.0152\n",
    "# Sparse EM GMM adjusted Rand Index: 0.6679 ± 0.0246\n",
    "# Sparse EM GMM Silhouette Score: 0.4422 ± 0.0133\n",
    "\n",
    "\n",
    "# log p_z\n",
    "# Averaged over 5 seeds:\n",
    "\n",
    "# Sparse EM GMM adjusted MI: 0.6360 ± 0.0167\n",
    "# Sparse EM GMM adjusted Rand Index: 0.4763 ± 0.0372\n",
    "# Sparse EM GMM Silhouette Score: 0.3929 ± 0.0413"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
